{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vishesh711/NLP-HW3/blob/main/hw3_code_skeleton_Dont_know%3F.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install the necessary libraries\n"
      ],
      "metadata": {
        "id": "57H-mVBq1d4v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "QCdFoYit1Iis"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "! pip install tqdm boto3 requests regex sentencepiece sacremoses\n",
        "! pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BERT Features\n",
        "\n",
        "In this part, you will use BERT features to classify DBPedia articles.\n",
        "The data is already pre-processed, and the data loader is implemented below."
      ],
      "metadata": {
        "id": "y2usTWPt2Tjs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Basics: dataset, data loaders, Classifier\n",
        "import collections\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import tqdm\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "SPLITS = ['train', 'dev', 'test']\n",
        "\n",
        "class DBPediaDataset(Dataset):\n",
        "  '''DBPedia dataset.\n",
        "    Args:\n",
        "      path[str]: path to the original data.\n",
        "  '''\n",
        "  def __init__(self, path):\n",
        "    with open(path) as fin:\n",
        "      self._data = [json.loads(l) for l in fin]\n",
        "    self._n_classes = len(set([datum['label'] for datum in self._data]))\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self._data[index]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self._data)\n",
        "\n",
        "  @property\n",
        "  def n_classes(self):\n",
        "    return self._n_classes\n",
        "\n",
        "  @staticmethod\n",
        "  def collate_fn(tokenizer, device, batch):\n",
        "    '''The collate function that compresses a training batch.\n",
        "      Args:\n",
        "        batch[list[dict[str, Any]]]: data in the batch.\n",
        "      Returns:\n",
        "        labels[torch.LongTensor]: the labels in the batch.\n",
        "        sentences[dict[str, torch.Tensor]]: sentences converted by tokenizers.\n",
        "    '''\n",
        "    labels = torch.tensor([datum['label'] for datum in batch]).long().to(device)\n",
        "    sentences = tokenizer(\n",
        "        [datum['sentence'] for datum in batch],\n",
        "        return_tensors='pt',  # pt = pytorch style tensor\n",
        "        padding=True)\n",
        "    for key in sentences:\n",
        "      sentences[key] = sentences[key].to(device)\n",
        "    return labels, sentences\n",
        "\n",
        "def construct_datasets(prefix, batch_size, tokenizer, device):\n",
        "  '''Constructs datasets and data loaders.\n",
        "    Args:\n",
        "      prefix[str]: prefix of the dataset (e.g., dbpedia_).\n",
        "      batch_size[int]: maximum number of examples in a batch.\n",
        "      tokenizer: model tokenizer that converts sentences to integer tensors.\n",
        "      device[torch.device]: the device (cpu/gpu) that the tensor should be on.\n",
        "    Returns:\n",
        "      datasets[dict[str, Dataset]]: a dict of constructed datasets.\n",
        "      dataloaders[dict[str, DataLoader]]: a dict of constructed data loaders.\n",
        "  '''\n",
        "  datasets = collections.defaultdict()\n",
        "  dataloaders = collections.defaultdict()\n",
        "  for split in SPLITS:\n",
        "    datasets[split] = DBPediaDataset(f'{prefix}{split}.json')\n",
        "    dataloaders[split] = DataLoader(\n",
        "        datasets[split],\n",
        "        batch_size=batch_size,\n",
        "        shuffle=(split == 'train'),\n",
        "        collate_fn=lambda x:DBPediaDataset.collate_fn(tokenizer, device, x))\n",
        "  return datasets, dataloaders"
      ],
      "metadata": {
        "id": "Sr_s2OH017B3"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.1: [CODE] put your implementation of classifer here\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(Classifier, self).__init__()\n",
        "        # First linear layer (input_size -> hidden_size)\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        # ReLU activation function\n",
        "        self.relu = nn.ReLU()\n",
        "        # Second linear layer (hidden_size -> num_classes)\n",
        "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pass input through first linear layer\n",
        "        x = self.fc1(x)\n",
        "        # Apply ReLU activation\n",
        "        x = self.relu(x)\n",
        "        # Pass through second linear layer to get logits\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "x7zw6kKUE4FY"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and Evaluation"
      ],
      "metadata": {
        "id": "isBytE8_OUSX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.1"
      ],
      "metadata": {
        "id": "YjUkVES2XaBZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Q1.1\")\n",
        "# hyperparameters\n",
        "batch_size = 32\n",
        "classifier_hidden_size = 32\n",
        "# hyperparameters ends\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
        "bert_model = AutoModel.from_pretrained('bert-base-cased')\n",
        "if torch.cuda.is_available():  # use GPU if available\n",
        "  bert_model = bert_model.cuda()\n",
        "datasets, dataloaders = construct_datasets(\n",
        "    prefix='dbpedia_',\n",
        "    batch_size=batch_size,\n",
        "    tokenizer=tokenizer,\n",
        "    device=bert_model.device)\n",
        "\n",
        "classifier = Classifier(\n",
        "    bert_model.config.hidden_size,\n",
        "    classifier_hidden_size,\n",
        "    datasets['train'].n_classes).to(bert_model.device)\n",
        "optimizer = torch.optim.Adam(classifier.parameters(), lr=5e-4)\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "pbar = tqdm.tqdm(dataloaders['train'])\n",
        "for labels, sentences in pbar:\n",
        "  with torch.no_grad():\n",
        "    unpooled_features = bert_model(**sentences)['last_hidden_state'] # [B, L, D]\n",
        "  # 1.1: [CODE] train your classifier here\n",
        "import random\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Function to set random seeds for reproducibility\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# Lists to store development accuracies and test accuracies\n",
        "dev_accuracies = []\n",
        "test_accuracies = []\n",
        "# List of seed values to use\n",
        "seed_values = [42, 123, 999, 2021, 7]  # Example seeds\n",
        "best_dev_accuracy = 0.0\n",
        "best_model_state = None\n",
        "\n",
        "# Loop over each seed value\n",
        "for seed in seed_values:\n",
        "    set_seed(seed)\n",
        "    # Initialize the classifier for each run\n",
        "    classifier = Classifier(\n",
        "        input_size=bert_model.config.hidden_size,  # Input size is 768 for BERT base\n",
        "        hidden_size=classifier_hidden_size,        # Hidden layer size as per requirement\n",
        "        num_classes=datasets['train'].n_classes    # Number of classes (14)\n",
        "    ).to(bert_model.device)\n",
        "    # Initialize the optimizer with classifier parameters\n",
        "    optimizer = torch.optim.Adam(classifier.parameters(), lr=5e-4)\n",
        "    # Define the loss function\n",
        "    loss_func = nn.CrossEntropyLoss()\n",
        "    # Training loop for one epoch\n",
        "    pbar = tqdm.tqdm(dataloaders['train'])\n",
        "    for labels, sentences in pbar:\n",
        "        # Use torch.no_grad() since BERT parameters are frozen\n",
        "        with torch.no_grad():\n",
        "            # Get unpooled features from BERT (batch_size, seq_length, hidden_size)\n",
        "            unpooled_features = bert_model(**sentences)['last_hidden_state']\n",
        "        # 1.1: [CODE] train your classifier here\n",
        "        # Extract the [CLS] token representation (batch_size, hidden_size)\n",
        "        cls_embeddings = unpooled_features[:, 0, :]\n",
        "        # Forward pass through the classifier to get logits\n",
        "        logits = classifier(cls_embeddings)\n",
        "        # Compute the cross-entropy loss between logits and labels\n",
        "        loss = loss_func(logits, labels)\n",
        "        # Backpropagation and optimization steps\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # Update progress bar with current loss\n",
        "        pbar.set_description(f\"Seed: {seed} | Loss: {loss.item():.4f}\")\n",
        "    # Evaluate the model on the development set\n",
        "    classifier.eval()  # Set classifier to evaluation mode\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for labels, sentences in dataloaders['dev']:\n",
        "            # Get BERT features\n",
        "            unpooled_features = bert_model(**sentences)['last_hidden_state']\n",
        "            # Extract [CLS] token representation\n",
        "            cls_embeddings = unpooled_features[:, 0, :]\n",
        "            # Get logits from classifier\n",
        "            logits = classifier(cls_embeddings)\n",
        "            # Get predicted classes\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "    # Calculate accuracy on the development set\n",
        "    dev_accuracy = accuracy_score(all_labels, all_preds)\n",
        "    dev_accuracies.append(dev_accuracy)\n",
        "    print(f\"Seed: {seed} | Dev Accuracy: {dev_accuracy:.4f}\")\n",
        "\n",
        "    # Save the best model based on dev accuracy\n",
        "    if dev_accuracy > best_dev_accuracy:\n",
        "        best_dev_accuracy = dev_accuracy\n",
        "        best_model_state = classifier.state_dict()\n",
        "        best_seed = seed\n",
        "\n",
        "# Compute mean and standard deviation of development accuracies\n",
        "mean_dev_accuracy = np.mean(dev_accuracies)\n",
        "std_dev_accuracy = np.std(dev_accuracies)\n",
        "print(f\"\\nMean Dev Accuracy: {mean_dev_accuracy:.4f}\")\n",
        "print(f\"Std Dev Accuracy: {std_dev_accuracy:.4f}\")\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "# Load the best model state\n",
        "classifier.load_state_dict(best_model_state)\n",
        "classifier.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "with torch.no_grad():\n",
        "    for labels, sentences in dataloaders['test']:\n",
        "        # Get BERT features\n",
        "        unpooled_features = bert_model(**sentences)['last_hidden_state']\n",
        "        # Extract [CLS] token representation\n",
        "        cls_embeddings = unpooled_features[:, 0, :]\n",
        "        # Get logits from classifier\n",
        "        logits = classifier(cls_embeddings)\n",
        "        # Get predicted classes\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "# Calculate accuracy on the test set\n",
        "test_accuracy = accuracy_score(all_labels, all_preds)\n",
        "print(f\"\\nBest Model Seed: {best_seed} | Test Accuracy: {test_accuracy:.4f}\")\n",
        "  # 1.1: [CODE] ends here\n",
        "print(\"******************************************************************************\")\n",
        "print(\"Q1.2\")\n",
        "#------------------------------------------------------------------------------------------------------------------------\n",
        "  # Note: you can re-use this code snippet for 1.2 as well\n",
        "class Classifier(nn.Module):\n",
        "    # Added code for 1.2 starts here\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(Classifier, self).__init__()\n",
        "        # First linear layer (input_size -> hidden_size)\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        # ReLU activation function\n",
        "        self.relu = nn.ReLU()\n",
        "        # Second linear layer (hidden_size -> num_classes)\n",
        "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pass input through first linear layer\n",
        "        x = self.fc1(x)\n",
        "        # Apply ReLU activation\n",
        "        x = self.relu(x)\n",
        "        # Pass through second linear layer to get logits\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "    # Added code for 1.2 ends here\n",
        "\n",
        "# Training and Evaluation\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 32\n",
        "classifier_hidden_size = 32\n",
        "# hyperparameters ends\n",
        "\n",
        "# Initialize tokenizer and BERT model\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
        "bert_model = AutoModel.from_pretrained('bert-base-cased')\n",
        "if torch.cuda.is_available():  # use GPU if available\n",
        "    bert_model = bert_model.cuda()\n",
        "\n",
        "# Construct datasets and data loaders\n",
        "datasets, dataloaders = construct_datasets(\n",
        "    prefix='dbpedia_',\n",
        "    batch_size=batch_size,\n",
        "    tokenizer=tokenizer,\n",
        "    device=bert_model.device\n",
        ")\n",
        "\n",
        "# Import necessary modules for setting random seeds and evaluation\n",
        "import random\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Function to set random seeds for reproducibility\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# Lists to store development accuracies and test accuracies\n",
        "dev_accuracies = []\n",
        "test_accuracies = []\n",
        "# List of seed values to use\n",
        "seed_values = [42, 123, 999, 2021, 7]  # Example seeds\n",
        "best_dev_accuracy = 0.0\n",
        "best_model_state = None\n",
        "\n",
        "# Loop over each seed value\n",
        "for seed in seed_values:\n",
        "    set_seed(seed)\n",
        "    # Initialize the classifier for each run\n",
        "\n",
        "    # 1.2: [CODE] Adjust the input size of the classifier\n",
        "    # Since we are concatenating mean-pooled and max-pooled vectors, each of size 768,\n",
        "    # the total input size becomes 768 * 2 = 1536\n",
        "    classifier_input_size = bert_model.config.hidden_size * 2  # 768 * 2 = 1536\n",
        "\n",
        "    classifier = Classifier(\n",
        "        input_size=classifier_input_size,\n",
        "        hidden_size=classifier_hidden_size,\n",
        "        num_classes=datasets['train'].n_classes\n",
        "    ).to(bert_model.device)\n",
        "\n",
        "    # Initialize the optimizer with classifier parameters\n",
        "    optimizer = torch.optim.Adam(classifier.parameters(), lr=5e-4)\n",
        "    # Define the loss function\n",
        "    loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Training loop for one epoch\n",
        "    pbar = tqdm.tqdm(dataloaders['train'])\n",
        "    for labels, sentences in pbar:\n",
        "        # Use torch.no_grad() since BERT parameters are frozen\n",
        "        with torch.no_grad():\n",
        "            # Get unpooled features from BERT\n",
        "            outputs = bert_model(**sentences)\n",
        "            unpooled_features = outputs['last_hidden_state']  # Shape: [B, L, D]\n",
        "\n",
        "            # 1.2: [CODE] Compute mean and max pooling over content tokens\n",
        "            # Added code starts here\n",
        "\n",
        "            # Get the attention mask to identify content tokens\n",
        "            attention_mask = sentences['attention_mask']  # Shape: [B, L]\n",
        "\n",
        "            # Expand attention mask to match the dimensions of unpooled_features\n",
        "            # We need to unsqueeze and expand it so that it can be used to mask the embeddings\n",
        "            # Mask shape will be [B, L, D]\n",
        "            mask_expanded = attention_mask.unsqueeze(-1).expand(unpooled_features.size()).float()\n",
        "\n",
        "            # Apply the attention mask to the unpooled features\n",
        "            # This zeroes out the embeddings of padding tokens\n",
        "            masked_embeddings = unpooled_features * mask_expanded  # Shape: [B, L, D]\n",
        "\n",
        "            # Compute mean pooling\n",
        "            # Sum the embeddings along the sequence length dimension\n",
        "            sum_embeddings = torch.sum(masked_embeddings, dim=1)  # Shape: [B, D]\n",
        "            # Sum the attention mask to get the number of valid tokens for each sample\n",
        "            sum_mask = torch.clamp(mask_expanded.sum(dim=1), min=1e-9)  # Shape: [B, D]\n",
        "            # Compute the mean by dividing summed embeddings by the number of valid tokens\n",
        "            mean_pooled = sum_embeddings / sum_mask  # Shape: [B, D]\n",
        "\n",
        "            # Compute max pooling\n",
        "            # For max pooling, we need to replace zeros in masked_embeddings with a very small value\n",
        "            # so that they do not affect the max operation\n",
        "            masked_embeddings[mask_expanded == 0] = -1e9  # Replace zeros with large negative number\n",
        "            # Compute the max over the sequence length dimension\n",
        "            max_pooled = torch.max(masked_embeddings, dim=1)[0]  # Shape: [B, D]\n",
        "\n",
        "            # Concatenate mean-pooled and max-pooled vectors\n",
        "            pooled_features = torch.cat((mean_pooled, max_pooled), dim=1)  # Shape: [B, D * 2]\n",
        "\n",
        "            # Added code ends here\n",
        "\n",
        "        # Forward pass through the classifier to get logits\n",
        "        logits = classifier(pooled_features)\n",
        "        # Compute the cross-entropy loss between logits and labels\n",
        "        loss = loss_func(logits, labels)\n",
        "        # Backpropagation and optimization steps\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # Update progress bar with current loss\n",
        "        pbar.set_description(f\"Seed: {seed} | Loss: {loss.item():.4f}\")\n",
        "\n",
        "    # Evaluate the model on the development set\n",
        "    classifier.eval()  # Set classifier to evaluation mode\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for labels, sentences in dataloaders['dev']:\n",
        "            # Get unpooled features from BERT\n",
        "            outputs = bert_model(**sentences)\n",
        "            unpooled_features = outputs['last_hidden_state']  # Shape: [B, L, D]\n",
        "\n",
        "            # 1.2: [CODE] Compute mean and max pooling over content tokens\n",
        "            # Same code as in training\n",
        "            # Added code starts here\n",
        "\n",
        "            # Get the attention mask\n",
        "            attention_mask = sentences['attention_mask']  # Shape: [B, L]\n",
        "\n",
        "            # Expand attention mask\n",
        "            mask_expanded = attention_mask.unsqueeze(-1).expand(unpooled_features.size()).float()\n",
        "\n",
        "            # Apply the attention mask\n",
        "            masked_embeddings = unpooled_features * mask_expanded  # Shape: [B, L, D]\n",
        "\n",
        "            # Compute mean pooling\n",
        "            sum_embeddings = torch.sum(masked_embeddings, dim=1)  # Shape: [B, D]\n",
        "            sum_mask = torch.clamp(mask_expanded.sum(dim=1), min=1e-9)  # Shape: [B, D]\n",
        "            mean_pooled = sum_embeddings / sum_mask  # Shape: [B, D]\n",
        "\n",
        "            # Compute max pooling\n",
        "            masked_embeddings[mask_expanded == 0] = -1e9  # Replace zeros with large negative number\n",
        "            max_pooled = torch.max(masked_embeddings, dim=1)[0]  # Shape: [B, D]\n",
        "\n",
        "            # Concatenate mean-pooled and max-pooled vectors\n",
        "            pooled_features = torch.cat((mean_pooled, max_pooled), dim=1)  # Shape: [B, D * 2]\n",
        "\n",
        "            # Added code ends here\n",
        "\n",
        "            # Get logits from classifier\n",
        "            logits = classifier(pooled_features)\n",
        "            # Get predicted classes\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Calculate accuracy on the development set\n",
        "    dev_accuracy = accuracy_score(all_labels, all_preds)\n",
        "    dev_accuracies.append(dev_accuracy)\n",
        "    print(f\"Seed: {seed} | Dev Accuracy: {dev_accuracy:.4f}\")\n",
        "\n",
        "    # Save the best model based on dev accuracy\n",
        "    if dev_accuracy > best_dev_accuracy:\n",
        "        best_dev_accuracy = dev_accuracy\n",
        "        best_model_state = classifier.state_dict()\n",
        "        best_seed = seed\n",
        "\n",
        "# Compute mean and standard deviation of development accuracies\n",
        "mean_dev_accuracy = np.mean(dev_accuracies)\n",
        "std_dev_accuracy = np.std(dev_accuracies)\n",
        "print(f\"\\nMean Dev Accuracy: {mean_dev_accuracy:.4f}\")\n",
        "print(f\"Std Dev Accuracy: {std_dev_accuracy:.4f}\")\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "# Load the best model state\n",
        "classifier.load_state_dict(best_model_state)\n",
        "classifier.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "with torch.no_grad():\n",
        "    for labels, sentences in dataloaders['test']:\n",
        "        # Get unpooled features from BERT\n",
        "        outputs = bert_model(**sentences)\n",
        "        unpooled_features = outputs['last_hidden_state']  # Shape: [B, L, D]\n",
        "\n",
        "        # 1.2: [CODE] Compute mean and max pooling over content tokens\n",
        "        # Same code as in training\n",
        "        # Added code starts here\n",
        "\n",
        "        # Get the attention mask\n",
        "        attention_mask = sentences['attention_mask']  # Shape: [B, L]\n",
        "\n",
        "        # Expand attention mask\n",
        "        mask_expanded = attention_mask.unsqueeze(-1).expand(unpooled_features.size()).float()\n",
        "\n",
        "        # Apply the attention mask\n",
        "        masked_embeddings = unpooled_features * mask_expanded  # Shape: [B, L, D]\n",
        "\n",
        "        # Compute mean pooling\n",
        "        sum_embeddings = torch.sum(masked_embeddings, dim=1)  # Shape: [B, D]\n",
        "        sum_mask = torch.clamp(mask_expanded.sum(dim=1), min=1e-9)  # Shape: [B, D]\n",
        "        mean_pooled = sum_embeddings / sum_mask  # Shape: [B, D]\n",
        "\n",
        "        # Compute max pooling\n",
        "        masked_embeddings[mask_expanded == 0] = -1e9  # Replace zeros with large negative number\n",
        "        max_pooled = torch.max(masked_embeddings, dim=1)[0]  # Shape: [B, D]\n",
        "\n",
        "        # Concatenate mean-pooled and max-pooled vectors\n",
        "        pooled_features = torch.cat((mean_pooled, max_pooled), dim=1)  # Shape: [B, D * 2]\n",
        "\n",
        "        # Added code ends here\n",
        "\n",
        "        # Get logits from classifier\n",
        "        logits = classifier(pooled_features)\n",
        "        # Get predicted classes\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Calculate accuracy on the test set\n",
        "test_accuracy = accuracy_score(all_labels, all_preds)\n",
        "print(f\"\\nBest Model Seed: {best_seed} | Test Accuracy: {test_accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "__xK_d_3pHuT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "585dd608-821b-4af6-e23e-8d60e744c2c4"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Seed: 2021 | Loss: 0.8122: 100%|██████████| 313/313 [00:41<00:00,  7.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed: 2021 | Dev Accuracy: 0.9310\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Seed: 7 | Loss: 0.7101: 100%|██████████| 313/313 [00:41<00:00,  7.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed: 7 | Dev Accuracy: 0.9080\n",
            "\n",
            "Mean Dev Accuracy: 0.9264\n",
            "Std Dev Accuracy: 0.0133\n",
            "\n",
            "Best Model Seed: 123 | Test Accuracy: 0.9320\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 313/313 [00:40<00:00,  7.74it/s]\n",
            "Seed: 42 | Loss: 0.5374: 100%|██████████| 313/313 [00:41<00:00,  7.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed: 42 | Dev Accuracy: 0.9620\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Seed: 123 | Loss: 0.2741: 100%|██████████| 313/313 [00:41<00:00,  7.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed: 123 | Dev Accuracy: 0.9620\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Seed: 999 | Loss: 0.3493: 100%|██████████| 313/313 [00:41<00:00,  7.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed: 999 | Dev Accuracy: 0.9690\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Seed: 2021 | Loss: 0.4027: 100%|██████████| 313/313 [00:41<00:00,  7.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed: 2021 | Dev Accuracy: 0.9610\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Seed: 7 | Loss: 0.4847: 100%|██████████| 313/313 [00:41<00:00,  7.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed: 7 | Dev Accuracy: 0.9080\n",
            "\n",
            "Mean Dev Accuracy: 0.9524\n",
            "Std Dev Accuracy: 0.0224\n",
            "\n",
            "Best Model Seed: 999 | Test Accuracy: 0.9740\n",
            "******************************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Seed: 42 | Loss: 0.4006: 100%|██████████| 313/313 [00:41<00:00,  7.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed: 42 | Dev Accuracy: 0.9360\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Seed: 123 | Loss: 0.6728: 100%|██████████| 313/313 [00:41<00:00,  7.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed: 123 | Dev Accuracy: 0.9430\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Seed: 999 | Loss: 0.8128: 100%|██████████| 313/313 [00:41<00:00,  7.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed: 999 | Dev Accuracy: 0.9140\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Seed: 2021 | Loss: 0.8122: 100%|██████████| 313/313 [00:41<00:00,  7.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed: 2021 | Dev Accuracy: 0.9310\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Seed: 7 | Loss: 0.7101: 100%|██████████| 313/313 [00:41<00:00,  7.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed: 7 | Dev Accuracy: 0.9080\n",
            "\n",
            "Mean Dev Accuracy: 0.9264\n",
            "Std Dev Accuracy: 0.0133\n",
            "\n",
            "Best Model Seed: 123 | Test Accuracy: 0.9320\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Q1.3\")\n",
        "# hyperparameters\n",
        "batch_size = 32\n",
        "classifier_hidden_size = 32\n",
        "# hyperparameters ends\n",
        "\n",
        "classifier = Classifier(\n",
        "    bert_model.config.hidden_size,\n",
        "    classifier_hidden_size,\n",
        "    datasets['train'].n_classes).to(bert_model.device)\n",
        "\n",
        "params = list()\n",
        "for name, param in bert_model.named_parameters():\n",
        "    if name.startswith('encoder.layer.10') or name.startswith('encoder.layer.11'):\n",
        "        param.requires_grad = True\n",
        "    else:\n",
        "        param.requires_grad = False\n",
        "\n",
        "# Collect parameters to optimize without duplicates\n",
        "params_to_optimize = []\n",
        "\n",
        "# Add classifier parameters\n",
        "classifier_params = list(classifier.parameters())\n",
        "params_to_optimize.extend(classifier_params)\n",
        "\n",
        "# Add BERT parameters that require gradients\n",
        "bert_params = [param for param in bert_model.parameters() if param.requires_grad]\n",
        "params_to_optimize.extend(bert_params)\n",
        "\n",
        "# Ensure no duplicates by checking parameter IDs\n",
        "seen = set()\n",
        "unique_params = []\n",
        "for param in params_to_optimize:\n",
        "    if id(param) not in seen:\n",
        "        unique_params.append(param)\n",
        "        seen.add(id(param))\n",
        "\n",
        "optimizer = torch.optim.Adam(unique_params, lr=5e-5)\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "pbar = tqdm.tqdm(dataloaders['train'])\n",
        "for labels, sentences in pbar:\n",
        "    # No torch.no_grad() since we are fine-tuning some BERT parameters\n",
        "    outputs = bert_model(**sentences)\n",
        "    unpooled_features = outputs['last_hidden_state']  # Shape: [B, L, D]\n",
        "\n",
        "    # Extract the [CLS] token representation\n",
        "    cls_features = unpooled_features[:, 0, :]  # Shape: [B, D]\n",
        "\n",
        "    # Forward pass through the classifier\n",
        "    logits = classifier(cls_features)\n",
        "\n",
        "    # Compute loss\n",
        "    loss = loss_func(logits, labels)\n",
        "\n",
        "    # Backpropagation and optimization\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Update progress bar with current loss\n",
        "    pbar.set_description(f\"Loss: {loss.item():.4f}\")\n",
        "print(\"***************************************************************************\")\n",
        "print(\"Q1.4\")\n",
        "# Finish your code here for 1.4. You may re-used most of your code for 1.1.\n",
        "# Lists to store development accuracies and test accuracies\n",
        "dev_accuracies = []\n",
        "test_accuracies = []\n",
        "# List of seed values to use\n",
        "seed_values = [42, 123, 999, 2021, 7]  # Example seeds\n",
        "best_dev_accuracy = 0.0\n",
        "best_model_state = None\n",
        "\n",
        "# Loop over each seed value\n",
        "for seed in seed_values:\n",
        "    set_seed(seed)\n",
        "    # Initialize the classifier for each run\n",
        "    classifier = Classifier(\n",
        "        input_size=bert_model.config.hidden_size,\n",
        "        hidden_size=classifier_hidden_size,\n",
        "        num_classes=datasets['train'].n_classes\n",
        "    ).to(bert_model.device)\n",
        "\n",
        "\n",
        "    # Set requires_grad for BERT parameters\n",
        "    for name, param in bert_model.named_parameters():\n",
        "        if name.startswith('encoder.layer.10.') or name.startswith('encoder.layer.11.'):\n",
        "            param.requires_grad = True  # Unfreeze last two layers\n",
        "        else:\n",
        "            param.requires_grad = False  # Freeze other layers\n",
        "\n",
        "    # Collect classifier parameters\n",
        "    classifier_params = list(classifier.parameters())\n",
        "\n",
        "    # Collect BERT parameters that require gradients (last two layers)\n",
        "    bert_params = [param for param in bert_model.parameters() if param.requires_grad]\n",
        "\n",
        "    # Combine parameters and ensure no duplicates\n",
        "    params_to_optimize = classifier_params + bert_params\n",
        "\n",
        "    # Initialize the optimizer\n",
        "    optimizer = torch.optim.Adam(params_to_optimize, lr=5e-5)  # Use a smaller learning rate for fine-tuning\n",
        "\n",
        "    # Define the loss function\n",
        "    loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Training loop for one epoch\n",
        "    pbar = tqdm.tqdm(dataloaders['train'])\n",
        "    for labels, sentences in pbar:\n",
        "        # No torch.no_grad() since we are fine-tuning some BERT parameters\n",
        "        # Get outputs from BERT\n",
        "        outputs = bert_model(**sentences)\n",
        "        unpooled_features = outputs['last_hidden_state']  # Shape: [B, L, D]\n",
        "\n",
        "        # Extract [CLS] token representation\n",
        "        cls_embeddings = unpooled_features[:, 0, :]  # Shape: [B, D]\n",
        "\n",
        "        # Forward pass through the classifier to get logits\n",
        "        logits = classifier(cls_embeddings)\n",
        "\n",
        "        # Compute the cross-entropy loss between logits and labels\n",
        "        loss = loss_func(logits, labels)\n",
        "\n",
        "        # Backpropagation and optimization steps\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update progress bar with current loss\n",
        "        pbar.set_description(f\"Seed: {seed} | Loss: {loss.item():.4f}\")\n",
        "\n",
        "    # Evaluate the model on the development set\n",
        "    classifier.eval()  # Set classifier to evaluation mode\n",
        "    bert_model.eval()  # Set BERT model to evaluation mode\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for labels, sentences in dataloaders['dev']:\n",
        "            # Get BERT features\n",
        "            outputs = bert_model(**sentences)\n",
        "            unpooled_features = outputs['last_hidden_state']  # Shape: [B, L, D]\n",
        "\n",
        "            # Extract [CLS] token representation\n",
        "            cls_embeddings = unpooled_features[:, 0, :]  # Shape: [B, D]\n",
        "\n",
        "            # Get logits from classifier\n",
        "            logits = classifier(cls_embeddings)\n",
        "\n",
        "            # Get predicted classes\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Calculate accuracy on the development set\n",
        "    dev_accuracy = accuracy_score(all_labels, all_preds)\n",
        "    dev_accuracies.append(dev_accuracy)\n",
        "    print(f\"Seed: {seed} | Dev Accuracy: {dev_accuracy:.4f}\")\n",
        "\n",
        "    # Save the best model based on dev accuracy\n",
        "    if dev_accuracy > best_dev_accuracy:\n",
        "        best_dev_accuracy = dev_accuracy\n",
        "        best_model_state = {\n",
        "            'classifier': classifier.state_dict(),\n",
        "            'bert_model': bert_model.state_dict()\n",
        "        }\n",
        "        best_seed = seed\n",
        "\n",
        "    # Set models back to training mode\n",
        "    classifier.train()\n",
        "    bert_model.train()\n",
        "\n",
        "# Compute mean and standard deviation of development accuracies\n",
        "mean_dev_accuracy = np.mean(dev_accuracies)\n",
        "std_dev_accuracy = np.std(dev_accuracies)\n",
        "print(f\"\\nMean Dev Accuracy: {mean_dev_accuracy:.4f}\")\n",
        "print(f\"Std Dev Accuracy: {std_dev_accuracy:.4f}\")\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "# Load the best model state\n",
        "classifier.load_state_dict(best_model_state['classifier'])\n",
        "bert_model.load_state_dict(best_model_state['bert_model'])\n",
        "classifier.eval()\n",
        "bert_model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "with torch.no_grad():\n",
        "    for labels, sentences in dataloaders['test']:\n",
        "        # Get BERT features\n",
        "        outputs = bert_model(**sentences)\n",
        "        unpooled_features = outputs['last_hidden_state']  # Shape: [B, L, D]\n",
        "\n",
        "        # Extract [CLS] token representation\n",
        "        cls_embeddings = unpooled_features[:, 0, :]  # Shape: [B, D]\n",
        "\n",
        "        # Get logits from classifier\n",
        "        logits = classifier(cls_embeddings)\n",
        "\n",
        "        # Get predicted classes\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Calculate accuracy on the test set\n",
        "test_accuracy = accuracy_score(all_labels, all_preds)\n",
        "print(f\"\\nBest Model Seed: {best_seed} | Test Accuracy: {test_accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "RV8KX_vEQUnn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "054f8e64-71b1-45c4-dfca-95dfafbed345"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q1.3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss: 0.0640: 100%|██████████| 313/313 [00:55<00:00,  5.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***************************************************************************\n",
            "Q1.4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Seed: 42 | Loss: 0.0321: 100%|██████████| 313/313 [00:53<00:00,  5.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed: 42 | Dev Accuracy: 0.9960\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Seed: 123 | Loss: 0.0343: 100%|██████████| 313/313 [00:54<00:00,  5.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed: 123 | Dev Accuracy: 0.9940\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Seed: 999 | Loss: 0.0324: 100%|██████████| 313/313 [00:54<00:00,  5.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed: 999 | Dev Accuracy: 0.9960\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Seed: 2021 | Loss: 0.0411: 100%|██████████| 313/313 [00:54<00:00,  5.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed: 2021 | Dev Accuracy: 0.9940\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Seed: 7 | Loss: 0.0438: 100%|██████████| 313/313 [00:54<00:00,  5.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed: 7 | Dev Accuracy: 0.9950\n",
            "\n",
            "Mean Dev Accuracy: 0.9950\n",
            "Std Dev Accuracy: 0.0009\n",
            "\n",
            "Best Model Seed: 42 | Test Accuracy: 0.0570\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameters\n",
        "batch_size = 32\n",
        "classifier_hidden_size = 32\n",
        "# hyperparameters ends\n",
        "\n",
        "# Initialize tokenizer and GPT-2 model\n",
        "from transformers import GPT2Tokenizer, GPT2Model\n",
        "\n",
        "# 1.5: [CODE] Replace BERT with GPT-2\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "# GPT-2 does not have padding token by default, so we need to add one\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "gpt2_model = GPT2Model.from_pretrained('gpt2')\n",
        "if torch.cuda.is_available():  # use GPU if available\n",
        "    gpt2_model = gpt2_model.cuda()\n",
        "\n",
        "# Construct datasets and data loaders\n",
        "datasets, dataloaders = construct_datasets(\n",
        "    prefix='dbpedia_',\n",
        "    batch_size=batch_size,\n",
        "    tokenizer=tokenizer,\n",
        "    device=gpt2_model.device\n",
        ")\n",
        "\n",
        "# Import necessary modules for setting random seeds and evaluation\n",
        "import random\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Function to set random seeds for reproducibility\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# Lists to store development accuracies and test accuracies\n",
        "dev_accuracies = []\n",
        "test_accuracies = []\n",
        "# List of seed values to use\n",
        "seed_values = [42, 123, 999, 2021, 7]  # Example seeds\n",
        "best_dev_accuracy = 0.0\n",
        "best_model_state = None\n",
        "\n",
        "# Loop over each seed value\n",
        "for seed in seed_values:\n",
        "    set_seed(seed)\n",
        "    # Initialize the classifier for each run\n",
        "    classifier = Classifier(\n",
        "        input_size=gpt2_model.config.hidden_size,  # GPT-2 hidden size is 768\n",
        "        hidden_size=classifier_hidden_size,\n",
        "        num_classes=datasets['train'].n_classes\n",
        "    ).to(gpt2_model.device)\n",
        "\n",
        "    # Initialize the optimizer with classifier parameters\n",
        "    optimizer = torch.optim.Adam(classifier.parameters(), lr=5e-4)\n",
        "    # Define the loss function\n",
        "    loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Training loop for one epoch\n",
        "    pbar = tqdm.tqdm(dataloaders['train'])\n",
        "    for labels, sentences in pbar:\n",
        "        # Use torch.no_grad() since GPT-2 parameters are frozen\n",
        "        with torch.no_grad():\n",
        "            # Get outputs from GPT-2\n",
        "            outputs = gpt2_model(**sentences)\n",
        "            # Get hidden states\n",
        "            hidden_states = outputs['last_hidden_state']  # Shape: [B, L, D]\n",
        "\n",
        "            # 1.5: [CODE] Extract features for classification\n",
        "            # Option 1: Use the last token's hidden state\n",
        "            # features = hidden_states[:, -1, :]  # Shape: [B, D]\n",
        "\n",
        "            # Option 2: Use mean pooling over all tokens\n",
        "            # Since GPT-2 is autoregressive, mean pooling can be effective\n",
        "            # Compute attention mask to exclude padding tokens\n",
        "            attention_mask = sentences['attention_mask'].unsqueeze(-1).expand(hidden_states.size()).float()\n",
        "            masked_hidden_states = hidden_states * attention_mask\n",
        "            sum_hidden_states = torch.sum(masked_hidden_states, dim=1)\n",
        "            sum_mask = attention_mask.sum(dim=1).clamp(min=1e-9)\n",
        "            features = sum_hidden_states / sum_mask  # Shape: [B, D]\n",
        "\n",
        "            # Note: You can choose either option. Here we use mean pooling.\n",
        "\n",
        "        # Forward pass through the classifier to get logits\n",
        "        logits = classifier(features)\n",
        "        # Compute the cross-entropy loss between logits and labels\n",
        "        loss = loss_func(logits, labels)\n",
        "        # Backpropagation and optimization steps\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # Update progress bar with current loss\n",
        "        pbar.set_description(f\"Seed: {seed} | Loss: {loss.item():.4f}\")\n",
        "\n",
        "    # Evaluate the model on the development set\n",
        "    classifier.eval()  # Set classifier to evaluation mode\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for labels, sentences in dataloaders['dev']:\n",
        "            # Get outputs from GPT-2\n",
        "            outputs = gpt2_model(**sentences)\n",
        "            hidden_states = outputs['last_hidden_state']  # Shape: [B, L, D]\n",
        "\n",
        "            # Extract features for classification (same as during training)\n",
        "            # Compute attention mask\n",
        "            attention_mask = sentences['attention_mask'].unsqueeze(-1).expand(hidden_states.size()).float()\n",
        "            masked_hidden_states = hidden_states * attention_mask\n",
        "            sum_hidden_states = torch.sum(masked_hidden_states, dim=1)\n",
        "            sum_mask = attention_mask.sum(dim=1).clamp(min=1e-9)\n",
        "            features = sum_hidden_states / sum_mask  # Shape: [B, D]\n",
        "\n",
        "            # Get logits from classifier\n",
        "            logits = classifier(features)\n",
        "            # Get predicted classes\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "    # Calculate accuracy on the development set\n",
        "    dev_accuracy = accuracy_score(all_labels, all_preds)\n",
        "    dev_accuracies.append(dev_accuracy)\n",
        "    print(f\"Seed: {seed} | Dev Accuracy: {dev_accuracy:.4f}\")\n",
        "\n",
        "    # Save the best model based on dev accuracy\n",
        "    if dev_accuracy > best_dev_accuracy:\n",
        "        best_dev_accuracy = dev_accuracy\n",
        "        best_model_state = classifier.state_dict()\n",
        "        best_seed = seed\n",
        "\n",
        "    # Set classifier back to training mode\n",
        "    classifier.train()\n",
        "\n",
        "# Compute mean and standard deviation of development accuracies\n",
        "mean_dev_accuracy = np.mean(dev_accuracies)\n",
        "std_dev_accuracy = np.std(dev_accuracies)\n",
        "print(f\"\\nMean Dev Accuracy: {mean_dev_accuracy:.4f}\")\n",
        "print(f\"Std Dev Accuracy: {std_dev_accuracy:.4f}\")\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "# Load the best model state\n",
        "classifier.load_state_dict(best_model_state)\n",
        "classifier.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "with torch.no_grad():\n",
        "    for labels, sentences in dataloaders['test']:\n",
        "        # Get outputs from GPT-2\n",
        "        outputs = gpt2_model(**sentences)\n",
        "        hidden_states = outputs['last_hidden_state']  # Shape: [B, L, D]\n",
        "\n",
        "        # Extract features for classification (same as during training)\n",
        "        # Compute attention mask\n",
        "        attention_mask = sentences['attention_mask'].unsqueeze(-1).expand(hidden_states.size()).float()\n",
        "        masked_hidden_states = hidden_states * attention_mask\n",
        "        sum_hidden_states = torch.sum(masked_hidden_states, dim=1)\n",
        "        sum_mask = attention_mask.sum(dim=1).clamp(min=1e-9)\n",
        "        features = sum_hidden_states / sum_mask  # Shape: [B, D]\n",
        "\n",
        "        # Get logits from classifier\n",
        "        logits = classifier(features)\n",
        "        # Get predicted classes\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "# Calculate accuracy on the test set\n",
        "test_accuracy = accuracy_score(all_labels, all_preds)\n",
        "print(f\"\\nBest Model Seed: {best_seed} | Test Accuracy: {test_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cSg84hfiXbuN",
        "outputId": "bd3d0b63-b062-4964-cef4-3e12c9b12c22"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Seed: 42 | Loss: 0.6046: 100%|██████████| 313/313 [00:46<00:00,  6.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed: 42 | Dev Accuracy: 0.8960\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Seed: 123 | Loss: 0.7952: 100%|██████████| 313/313 [00:43<00:00,  7.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed: 123 | Dev Accuracy: 0.8300\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Seed: 999 | Loss: 0.7159: 100%|██████████| 313/313 [00:44<00:00,  7.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed: 999 | Dev Accuracy: 0.8910\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Seed: 2021 | Loss: 0.6895: 100%|██████████| 313/313 [00:44<00:00,  7.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed: 2021 | Dev Accuracy: 0.8920\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Seed: 7 | Loss: 0.6747: 100%|██████████| 313/313 [00:44<00:00,  7.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed: 7 | Dev Accuracy: 0.8650\n",
            "\n",
            "Mean Dev Accuracy: 0.8748\n",
            "Std Dev Accuracy: 0.0249\n",
            "\n",
            "Best Model Seed: 42 | Test Accuracy: 0.8900\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e21h_u_JvLES"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}