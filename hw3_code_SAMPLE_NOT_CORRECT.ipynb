{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vishesh711/NLP-HW3/blob/main/hw3_code_SAMPLE_NOT_CORRECT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install the necessary libraries\n"
      ],
      "metadata": {
        "id": "57H-mVBq1d4v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "QCdFoYit1Iis"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "! pip install tqdm boto3 requests regex sentencepiece sacremoses\n",
        "! pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BERT Features\n",
        "\n",
        "In this part, you will use BERT features to classify DBPedia articles.\n",
        "The data is already pre-processed, and the data loader is implemented below."
      ],
      "metadata": {
        "id": "y2usTWPt2Tjs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Basics: dataset, data loaders, Classifier\n",
        "import collections\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import tqdm\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "\n",
        "SPLITS = ['train', 'dev', 'test']\n",
        "\n",
        "class DBPediaDataset(Dataset):\n",
        "  '''DBPedia dataset.\n",
        "    Args:\n",
        "      path[str]: path to the original data.\n",
        "  '''\n",
        "  def __init__(self, path):\n",
        "    with open(path) as fin:\n",
        "      self._data = [json.loads(l) for l in fin]\n",
        "    self._n_classes = len(set([datum['label'] for datum in self._data]))\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self._data[index]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self._data)\n",
        "\n",
        "  @property\n",
        "  def n_classes(self):\n",
        "    return self._n_classes\n",
        "\n",
        "  @staticmethod\n",
        "  def collate_fn(tokenizer, device, batch):\n",
        "    '''The collate function that compresses a training batch.\n",
        "      Args:\n",
        "        batch[list[dict[str, Any]]]: data in the batch.\n",
        "      Returns:\n",
        "        labels[torch.LongTensor]: the labels in the batch.\n",
        "        sentences[dict[str, torch.Tensor]]: sentences converted by tokenizers.\n",
        "    '''\n",
        "    labels = torch.tensor([datum['label'] for datum in batch]).long().to(device)\n",
        "    sentences = tokenizer(\n",
        "        [datum['sentence'] for datum in batch],\n",
        "        return_tensors='pt',  # pt = pytorch style tensor\n",
        "        padding=True)\n",
        "    for key in sentences:\n",
        "      sentences[key] = sentences[key].to(device)\n",
        "    return labels, sentences\n",
        "\n",
        "def construct_datasets(prefix, batch_size, tokenizer, device):\n",
        "  '''Constructs datasets and data loaders.\n",
        "    Args:\n",
        "      prefix[str]: prefix of the dataset (e.g., dbpedia_).\n",
        "      batch_size[int]: maximum number of examples in a batch.\n",
        "      tokenizer: model tokenizer that converts sentences to integer tensors.\n",
        "      device[torch.device]: the device (cpu/gpu) that the tensor should be on.\n",
        "    Returns:\n",
        "      datasets[dict[str, Dataset]]: a dict of constructed datasets.\n",
        "      dataloaders[dict[str, DataLoader]]: a dict of constructed data loaders.\n",
        "  '''\n",
        "  datasets = collections.defaultdict()\n",
        "  dataloaders = collections.defaultdict()\n",
        "  for split in SPLITS:\n",
        "    datasets[split] = DBPediaDataset(f'{prefix}{split}.json')\n",
        "    dataloaders[split] = DataLoader(\n",
        "        datasets[split],\n",
        "        batch_size=batch_size,\n",
        "        shuffle=(split == 'train'),\n",
        "        collate_fn=lambda x:DBPediaDataset.collate_fn(tokenizer, device, x))\n",
        "  return datasets, dataloaders"
      ],
      "metadata": {
        "id": "Sr_s2OH017B3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.1: [CODE] put your implementation of classifer here\n",
        "class Classifier(nn.Module):\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "gqQaEhdVYvvp",
        "outputId": "368586a6-d1a6-4213-8a87-2a938f3908f5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "incomplete input (<ipython-input-11-25a47fbfba18>, line 2)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-11-25a47fbfba18>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    class Classifier(nn.Module):\u001b[0m\n\u001b[0m                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#MY_CODE\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import numpy as np\n",
        "import random\n",
        "# 1.1: [CODE] put your implementation of classifer here\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "def set_random_seeds(seed_value):\n",
        "    torch.manual_seed(seed_value)\n",
        "    torch.cuda.manual_seed_all(seed_value)\n",
        "    np.random.seed(seed_value)\n",
        "    random.seed(seed_value)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 32\n",
        "classifier_hidden_size = 32\n",
        "learning_rate = 5e-4\n",
        "num_epochs = 1\n",
        "input_size = 768  # BERT [CLS] token size\n",
        "num_classes = 14\n",
        "\n",
        "# Initialize tokenizer and BERT model\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
        "bert_model = AutoModel.from_pretrained('bert-base-cased')\n",
        "if torch.cuda.is_available():\n",
        "    bert_model = bert_model.cuda()\n",
        "\n",
        "# Prepare data loaders\n",
        "datasets, dataloaders = construct_datasets(\n",
        "    prefix='dbpedia_',\n",
        "    batch_size=batch_size,\n",
        "    tokenizer=tokenizer,\n",
        "    device=bert_model.device\n",
        ")\n",
        "\n",
        "# Train and evaluate for multiple seeds\n",
        "num_runs = 5\n",
        "dev_accuracies = []\n",
        "best_test_accuracy = 0\n",
        "best_seed = None\n",
        "\n",
        "for run in range(num_runs):\n",
        "    # Set a different random seed for each run\n",
        "    seed = 42 + run  # or choose any other seed values\n",
        "    set_random_seeds(seed)\n",
        "\n",
        "    # Initialize the classifier and optimizer\n",
        "    classifier = Classifier(input_size, classifier_hidden_size, num_classes).to(bert_model.device)\n",
        "    optimizer = optim.Adam(classifier.parameters(), lr=learning_rate)\n",
        "    loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Training loop\n",
        "    bert_model.eval()  # Freeze BERT model\n",
        "    classifier.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        pbar = tqdm.tqdm(dataloaders['train'], desc=f\"Run {run+1}, Epoch {epoch+1}\")\n",
        "        for labels, sentences in pbar:\n",
        "            with torch.no_grad():  # Freeze BERT and extract CLS features\n",
        "                cls_features = bert_model(**sentences)['last_hidden_state'][:, 0, :]  # [CLS] token\n",
        "\n",
        "            # Forward pass through the classifier\n",
        "            outputs = classifier(cls_features)\n",
        "            loss = loss_func(outputs, labels)\n",
        "\n",
        "            # Backward and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # Evaluate on development set\n",
        "    classifier.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for labels, sentences in dataloaders['dev']:\n",
        "            cls_features = bert_model(**sentences)['last_hidden_state'][:, 0, :]\n",
        "            outputs = classifier(cls_features)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    dev_accuracy = correct / total\n",
        "    dev_accuracies.append(dev_accuracy)\n",
        "\n",
        "    # Check if this is the best model\n",
        "    if dev_accuracy > best_test_accuracy:\n",
        "        best_test_accuracy = dev_accuracy\n",
        "        best_seed = seed\n",
        "\n",
        "    print(f\"Run {run+1} - Dev Accuracy: {dev_accuracy:.4f}\")\n",
        "\n",
        "# Calculate mean and standard deviation for dev set accuracies\n",
        "mean_dev_accuracy = np.mean(dev_accuracies)\n",
        "std_dev_accuracy = np.std(dev_accuracies)\n",
        "print(f\"Mean Dev Accuracy: {mean_dev_accuracy:.4f}, Standard Deviation: {std_dev_accuracy:.4f}\")\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "set_random_seeds(best_seed)  # Set the seed for best model\n",
        "classifier = Classifier(input_size, classifier_hidden_size, num_classes).to(bert_model.device)\n",
        "optimizer = optim.Adam(classifier.parameters(), lr=learning_rate)\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "bert_model.eval()\n",
        "classifier.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for labels, sentences in dataloaders['test']:\n",
        "        cls_features = bert_model(**sentences)['last_hidden_state'][:, 0, :]\n",
        "        outputs = classifier(cls_features)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "test_accuracy = correct / total\n",
        "print(f\"Best Test Accuracy: {test_accuracy:.4f} (Seed: {best_seed})\")"
      ],
      "metadata": {
        "id": "x7zw6kKUE4FY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a87beee-99e1-4baf-d162-7b78ff21a273"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Run 1, Epoch 1: 100%|██████████| 313/313 [00:41<00:00,  7.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run 1 - Dev Accuracy: 0.9620\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Run 2, Epoch 1: 100%|██████████| 313/313 [00:42<00:00,  7.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run 2 - Dev Accuracy: 0.9600\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Run 3, Epoch 1: 100%|██████████| 313/313 [00:42<00:00,  7.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run 3 - Dev Accuracy: 0.9790\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Run 4, Epoch 1: 100%|██████████| 313/313 [00:42<00:00,  7.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run 4 - Dev Accuracy: 0.9760\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Run 5, Epoch 1: 100%|██████████| 313/313 [00:41<00:00,  7.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run 5 - Dev Accuracy: 0.9390\n",
            "Mean Dev Accuracy: 0.9632, Standard Deviation: 0.0142\n",
            "Best Test Accuracy: 0.1660 (Seed: 44)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Updated Classifier Model for Mean-Pooling and Max-Pooling (Input size is doubled)\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Mean and Max Pooling Function\n",
        "def mean_max_pooling(features, attention_mask):\n",
        "    attention_mask_expanded = attention_mask.unsqueeze(-1).expand(features.size()).float()\n",
        "\n",
        "    # Mean Pooling\n",
        "    sum_embeddings = torch.sum(features * attention_mask_expanded, dim=1)\n",
        "    sum_mask = torch.clamp(attention_mask_expanded.sum(dim=1), min=1e-9)  # Avoid division by zero\n",
        "    mean_pooled = sum_embeddings / sum_mask\n",
        "\n",
        "    # Max Pooling\n",
        "    features[attention_mask == 0] = -1e9  # Mask out padding tokens for max pooling\n",
        "    max_pooled, _ = torch.max(features, dim=1)\n",
        "\n",
        "    # Concatenate Mean and Max Pooling\n",
        "    pooled_output = torch.cat((mean_pooled, max_pooled), dim=1)\n",
        "    return pooled_output\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "def set_random_seeds(seed_value):\n",
        "    torch.manual_seed(seed_value)\n",
        "    torch.cuda.manual_seed_all(seed_value)\n",
        "    np.random.seed(seed_value)\n",
        "    random.seed(seed_value)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 32\n",
        "classifier_hidden_size = 32\n",
        "learning_rate = 5e-4\n",
        "num_epochs = 1\n",
        "input_size = 1536  # Doubled due to mean and max pooling\n",
        "num_classes = 14\n",
        "\n",
        "# Initialize tokenizer and BERT model\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
        "bert_model = AutoModel.from_pretrained('bert-base-cased')\n",
        "if torch.cuda.is_available():\n",
        "    bert_model = bert_model.cuda()\n",
        "\n",
        "# Prepare data loaders\n",
        "datasets, dataloaders = construct_datasets(\n",
        "    prefix='dbpedia_',\n",
        "    batch_size=batch_size,\n",
        "    tokenizer=tokenizer,\n",
        "    device=bert_model.device\n",
        ")\n",
        "\n",
        "# Train and evaluate for multiple seeds\n",
        "num_runs = 5\n",
        "dev_accuracies = []\n",
        "best_test_accuracy = 0\n",
        "best_seed = None\n",
        "\n",
        "for run in range(num_runs):\n",
        "    # Set a different random seed for each run\n",
        "    seed = 42 + run\n",
        "    set_random_seeds(seed)\n",
        "\n",
        "    # Initialize the classifier and optimizer\n",
        "    classifier = Classifier(input_size, classifier_hidden_size, num_classes).to(bert_model.device)\n",
        "    optimizer = optim.Adam(classifier.parameters(), lr=learning_rate)\n",
        "    loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Training loop\n",
        "    bert_model.eval()  # Freeze BERT model\n",
        "    classifier.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        pbar = tqdm.tqdm(dataloaders['train'], desc=f\"Run {run+1}, Epoch {epoch+1}\")\n",
        "        for labels, sentences in pbar:\n",
        "            with torch.no_grad():  # Freeze BERT and extract features\n",
        "                bert_outputs = bert_model(**sentences)\n",
        "                attention_mask = sentences['attention_mask']\n",
        "                cls_features = mean_max_pooling(bert_outputs['last_hidden_state'], attention_mask)  # Mean and Max Pooling\n",
        "\n",
        "            # Forward pass through the classifier\n",
        "            outputs = classifier(cls_features)\n",
        "            loss = loss_func(outputs, labels)\n",
        "\n",
        "            # Backward and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # Evaluate on development set\n",
        "    classifier.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for labels, sentences in dataloaders['dev']:\n",
        "            bert_outputs = bert_model(**sentences)\n",
        "            attention_mask = sentences['attention_mask']\n",
        "            cls_features = mean_max_pooling(bert_outputs['last_hidden_state'], attention_mask)\n",
        "            outputs = classifier(cls_features)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    dev_accuracy = correct / total\n",
        "    dev_accuracies.append(dev_accuracy)\n",
        "\n",
        "    # Check if this is the best model\n",
        "    if dev_accuracy > best_test_accuracy:\n",
        "        best_test_accuracy = dev_accuracy\n",
        "        best_seed = seed\n",
        "\n",
        "    print(f\"Run {run+1} - Dev Accuracy: {dev_accuracy:.4f}\")\n",
        "\n",
        "# Calculate mean and standard deviation for dev set accuracies\n",
        "mean_dev_accuracy = np.mean(dev_accuracies)\n",
        "std_dev_accuracy = np.std(dev_accuracies)\n",
        "print(f\"Mean Dev Accuracy: {mean_dev_accuracy:.4f}, Standard Deviation: {std_dev_accuracy:.4f}\")\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "set_random_seeds(best_seed)\n",
        "classifier = Classifier(input_size, classifier_hidden_size, num_classes).to(bert_model.device)\n",
        "optimizer = optim.Adam(classifier.parameters(), lr=learning_rate)\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "bert_model.eval()\n",
        "classifier.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for labels, sentences in dataloaders['test']:\n",
        "        bert_outputs = bert_model(**sentences)\n",
        "        attention_mask = sentences['attention_mask']\n",
        "        cls_features = mean_max_pooling(bert_outputs['last_hidden_state'], attention_mask)\n",
        "        outputs = classifier(cls_features)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "test_accuracy = correct / total\n",
        "print(f\"Best Test Accuracy: {test_accuracy:.4f} (Seed: {best_seed})\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1R8st6myY2cn",
        "outputId": "8e0e1e3b-7335-4648-cfe0-6920805e42d2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Run 1, Epoch 1: 100%|██████████| 313/313 [00:42<00:00,  7.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run 1 - Dev Accuracy: 0.9360\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Run 2, Epoch 1: 100%|██████████| 313/313 [00:41<00:00,  7.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run 2 - Dev Accuracy: 0.9360\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Run 3, Epoch 1: 100%|██████████| 313/313 [00:42<00:00,  7.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run 3 - Dev Accuracy: 0.9250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Run 4, Epoch 1: 100%|██████████| 313/313 [00:42<00:00,  7.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run 4 - Dev Accuracy: 0.9310\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Run 5, Epoch 1: 100%|██████████| 313/313 [00:42<00:00,  7.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run 5 - Dev Accuracy: 0.9340\n",
            "Mean Dev Accuracy: 0.9324, Standard Deviation: 0.0041\n",
            "Best Test Accuracy: 0.0710 (Seed: 42)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "def set_random_seeds(seed_value):\n",
        "    torch.manual_seed(seed_value)\n",
        "    torch.cuda.manual_seed_all(seed_value)\n",
        "    np.random.seed(seed_value)\n",
        "    random.seed(seed_value)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 32\n",
        "classifier_hidden_size = 32\n",
        "learning_rate = 5e-4\n",
        "num_epochs = 1\n",
        "input_size = 768  # First-token CLS size\n",
        "num_classes = 14\n",
        "\n",
        "# Initialize tokenizer and BERT model\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
        "bert_model = AutoModel.from_pretrained('bert-base-cased')\n",
        "if torch.cuda.is_available():\n",
        "    bert_model = bert_model.cuda()\n",
        "\n",
        "# Prepare data loaders\n",
        "datasets, dataloaders = construct_datasets(\n",
        "    prefix='dbpedia_',\n",
        "    batch_size=batch_size,\n",
        "    tokenizer=tokenizer,\n",
        "    device=bert_model.device\n",
        ")\n",
        "\n",
        "# Updated Classifier Model\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Train and evaluate for multiple seeds\n",
        "num_runs = 5\n",
        "dev_accuracies = []\n",
        "best_test_accuracy = 0\n",
        "best_seed = None\n",
        "\n",
        "for run in range(num_runs):\n",
        "    # Set a different random seed for each run\n",
        "    seed = 42 + run\n",
        "    set_random_seeds(seed)\n",
        "\n",
        "    # Initialize the classifier and optimizer\n",
        "    classifier = Classifier(input_size, classifier_hidden_size, num_classes).to(bert_model.device)\n",
        "\n",
        "    # Gather parameters for last two layers and classifier\n",
        "    params = list()\n",
        "    for name, param in bert_model.named_parameters():\n",
        "        if \"encoder.layer.10\" in name or \"encoder.layer.11\" in name:\n",
        "            param.requires_grad = True\n",
        "            params.append(param)\n",
        "        else:\n",
        "            param.requires_grad = False\n",
        "    optimizer = optim.Adam(params + list(classifier.parameters()), lr=learning_rate)\n",
        "    loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Training loop with fine-tuning of last two BERT layers\n",
        "    bert_model.train()  # Allow training of last two layers\n",
        "    classifier.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        pbar = tqdm.tqdm(dataloaders['train'], desc=f\"Run {run+1}, Epoch {epoch+1}\")\n",
        "        for labels, sentences in pbar:\n",
        "            # Extract CLS features from BERT with grad enabled for last two layers\n",
        "            cls_features = bert_model(**sentences)['last_hidden_state'][:, 0, :]\n",
        "\n",
        "            # Forward pass through the classifier\n",
        "            outputs = classifier(cls_features)\n",
        "            loss = loss_func(outputs, labels)\n",
        "\n",
        "            # Backward and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # Evaluate on development set\n",
        "    classifier.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for labels, sentences in dataloaders['dev']:\n",
        "            cls_features = bert_model(**sentences)['last_hidden_state'][:, 0, :]\n",
        "            outputs = classifier(cls_features)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    dev_accuracy = correct / total\n",
        "    dev_accuracies.append(dev_accuracy)\n",
        "\n",
        "    # Check if this is the best model\n",
        "    if dev_accuracy > best_test_accuracy:\n",
        "        best_test_accuracy = dev_accuracy\n",
        "        best_seed = seed\n",
        "\n",
        "    print(f\"Run {run+1} - Dev Accuracy: {dev_accuracy:.4f}\")\n",
        "\n",
        "# Calculate mean and standard deviation for dev set accuracies\n",
        "mean_dev_accuracy = np.mean(dev_accuracies)\n",
        "std_dev_accuracy = np.std(dev_accuracies)\n",
        "print(f\"Mean Dev Accuracy: {mean_dev_accuracy:.4f}, Standard Deviation: {std_dev_accuracy:.4f}\")\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "set_random_seeds(best_seed)\n",
        "classifier = Classifier(input_size, classifier_hidden_size, num_classes).to(bert_model.device)\n",
        "optimizer = optim.Adam(params + list(classifier.parameters()), lr=learning_rate)\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "bert_model.eval()\n",
        "classifier.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for labels, sentences in dataloaders['test']:\n",
        "        cls_features = bert_model(**sentences)['last_hidden_state'][:, 0, :]\n",
        "        outputs = classifier(cls_features)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "test_accuracy = correct / total\n",
        "print(f\"Best Test Accuracy: {test_accuracy:.4f} (Seed: {best_seed})\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hofg5F2ibMn2",
        "outputId": "0fc39475-c05e-4204-8b2f-f4e7394832ac"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Run 1, Epoch 1: 100%|██████████| 313/313 [00:52<00:00,  5.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run 1 - Dev Accuracy: 0.9750\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Run 2, Epoch 1: 100%|██████████| 313/313 [00:55<00:00,  5.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run 2 - Dev Accuracy: 0.9910\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Run 3, Epoch 1: 100%|██████████| 313/313 [00:54<00:00,  5.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run 3 - Dev Accuracy: 0.9850\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Run 4, Epoch 1: 100%|██████████| 313/313 [00:54<00:00,  5.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run 4 - Dev Accuracy: 0.9920\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Run 5, Epoch 1: 100%|██████████| 313/313 [00:54<00:00,  5.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run 5 - Dev Accuracy: 0.9890\n",
            "Mean Dev Accuracy: 0.9864, Standard Deviation: 0.0062\n",
            "Best Test Accuracy: 0.9820 (Seed: 45)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import numpy as np\n",
        "import random\n",
        "import tqdm\n",
        "\n",
        "# Updated Classifier Model\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "def set_random_seeds(seed_value):\n",
        "    torch.manual_seed(seed_value)\n",
        "    torch.cuda.manual_seed_all(seed_value)\n",
        "    np.random.seed(seed_value)\n",
        "    random.seed(seed_value)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 32\n",
        "classifier_hidden_size = 32\n",
        "learning_rate = 5e-4\n",
        "num_epochs = 1\n",
        "input_size = 768  # Hidden size for GPT-2\n",
        "num_classes = 14\n",
        "\n",
        "# Initialize tokenizer and GPT-2 model\n",
        "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
        "\n",
        "# Set padding token for GPT-2 tokenizer\n",
        "tokenizer.pad_token = tokenizer.eos_token  # Use eos_token as pad_token\n",
        "\n",
        "gpt2_model = AutoModel.from_pretrained('gpt2')\n",
        "if torch.cuda.is_available():\n",
        "    gpt2_model = gpt2_model.cuda()\n",
        "\n",
        "# Prepare data loaders\n",
        "datasets, dataloaders = construct_datasets(\n",
        "    prefix='dbpedia_',\n",
        "    batch_size=batch_size,\n",
        "    tokenizer=tokenizer,\n",
        "    device=gpt2_model.device\n",
        ")\n",
        "\n",
        "# Train and evaluate for multiple seeds\n",
        "num_runs = 5\n",
        "dev_accuracies = []\n",
        "best_test_accuracy = 0\n",
        "best_seed = None\n",
        "\n",
        "for run in range(num_runs):\n",
        "    # Set a different random seed for each run\n",
        "    seed = 42 + run\n",
        "    set_random_seeds(seed)\n",
        "\n",
        "    # Initialize the classifier and optimizer\n",
        "    classifier = Classifier(input_size, classifier_hidden_size, num_classes).to(gpt2_model.device)\n",
        "    optimizer = optim.Adam(classifier.parameters(), lr=learning_rate)\n",
        "    loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Training loop with frozen GPT-2\n",
        "    gpt2_model.eval()  # Freeze GPT-2\n",
        "    classifier.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        pbar = tqdm.tqdm(dataloaders['train'], desc=f\"Run {run+1}, Epoch {epoch+1}\")\n",
        "        for labels, sentences in pbar:\n",
        "            with torch.no_grad():  # Extract features from GPT-2\n",
        "                gpt2_outputs = gpt2_model(**sentences)\n",
        "                last_token_features = gpt2_outputs['last_hidden_state'][:, -1, :]  # Last token of each sequence\n",
        "\n",
        "            # Forward pass through the classifier\n",
        "            outputs = classifier(last_token_features)\n",
        "            loss = loss_func(outputs, labels)\n",
        "\n",
        "            # Backward and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # Evaluate on development set\n",
        "    classifier.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for labels, sentences in dataloaders['dev']:\n",
        "            gpt2_outputs = gpt2_model(**sentences)\n",
        "            last_token_features = gpt2_outputs['last_hidden_state'][:, -1, :]\n",
        "            outputs = classifier(last_token_features)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    dev_accuracy = correct / total\n",
        "    dev_accuracies.append(dev_accuracy)\n",
        "\n",
        "    # Check if this is the best model\n",
        "    if dev_accuracy > best_test_accuracy:\n",
        "        best_test_accuracy = dev_accuracy\n",
        "        best_seed = seed\n",
        "\n",
        "    print(f\"Run {run+1} - Dev Accuracy: {dev_accuracy:.4f}\")\n",
        "\n",
        "# Calculate mean and standard deviation for dev set accuracies\n",
        "mean_dev_accuracy = np.mean(dev_accuracies)\n",
        "std_dev_accuracy = np.std(dev_accuracies)\n",
        "print(f\"Mean Dev Accuracy: {mean_dev_accuracy:.4f}, Standard Deviation: {std_dev_accuracy:.4f}\")\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "set_random_seeds(best_seed)\n",
        "classifier = Classifier(input_size, classifier_hidden_size, num_classes).to(gpt2_model.device)\n",
        "optimizer = optim.Adam(classifier.parameters(), lr=learning_rate)\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "gpt2_model.eval()\n",
        "classifier.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for labels, sentences in dataloaders['test']:\n",
        "        gpt2_outputs = gpt2_model(**sentences)\n",
        "        last_token_features = gpt2_outputs['last_hidden_state'][:, -1, :]\n",
        "        outputs = classifier(last_token_features)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "test_accuracy = correct / total\n",
        "print(f\"Best Test Accuracy: {test_accuracy:.4f} (Seed: {best_seed})\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpeChxsAdYd1",
        "outputId": "d193fe0c-d5bd-425d-b84d-4e993310f362"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Run 1, Epoch 1: 100%|██████████| 313/313 [00:41<00:00,  7.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run 1 - Dev Accuracy: 0.3270\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Run 2, Epoch 1: 100%|██████████| 313/313 [00:44<00:00,  7.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run 2 - Dev Accuracy: 0.3030\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Run 3, Epoch 1: 100%|██████████| 313/313 [00:43<00:00,  7.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run 3 - Dev Accuracy: 0.4140\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Run 4, Epoch 1: 100%|██████████| 313/313 [00:43<00:00,  7.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run 4 - Dev Accuracy: 0.2660\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Run 5, Epoch 1: 100%|██████████| 313/313 [00:43<00:00,  7.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run 5 - Dev Accuracy: 0.3140\n",
            "Mean Dev Accuracy: 0.3248, Standard Deviation: 0.0490\n",
            "Best Test Accuracy: 0.0580 (Seed: 44)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Training and Evaluation"
      ],
      "metadata": {
        "id": "isBytE8_OUSX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameters\n",
        "batch_size = 32\n",
        "classifier_hidden_size = 32\n",
        "# hyperparameters ends\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
        "bert_model = AutoModel.from_pretrained('bert-base-cased')\n",
        "if torch.cuda.is_available():  # use GPU if available\n",
        "  bert_model = bert_model.cuda()\n",
        "datasets, dataloaders = construct_datasets(\n",
        "    prefix='dbpedia_',\n",
        "    batch_size=batch_size,\n",
        "    tokenizer=tokenizer,\n",
        "    device=bert_model.device)\n",
        "\n",
        "classifier = Classifier(\n",
        "    bert_model.config.hidden_size,\n",
        "    classifier_hidden_size,\n",
        "    datasets['train'].n_classes).to(bert_model.device)\n",
        "optimizer = torch.optim.Adam(classifier.parameters(), lr=5e-4)\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "pbar = tqdm.tqdm(dataloaders['train'])\n",
        "for labels, sentences in pbar:\n",
        "  with torch.no_grad():\n",
        "    unpooled_features = bert_model(**sentences)['last_hidden_state'] # [B, L, D]\n",
        "  # 1.1: [CODE] train your classifier here\n",
        "\n",
        "  # 1.1: [CODE] ends here\n",
        "  # Note: you can re-use this code snippet for 1.2 as well"
      ],
      "metadata": {
        "id": "__xK_d_3pHuT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameters\n",
        "batch_size = 32\n",
        "classifier_hidden_size = 32\n",
        "# hyperparameters ends\n",
        "\n",
        "classifier = Classifier(\n",
        "    bert_model.config.hidden_size,\n",
        "    classifier_hidden_size,\n",
        "    datasets['train'].n_classes).to(bert_model.device)\n",
        "\n",
        "params = list()\n",
        "for name, param in bert_model.named_parameters():\n",
        "  if name.startswith... # 1.3: [CODE] this line is incomplete, you can finish this line by adding the last two layers' parameters to \"params\", or re-write your own code\n",
        "    params.append(param)\n",
        "optimizer = torch.optim.Adam(params + list(classifier.parameters()), lr=5e-4)\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "pbar = tqdm.tqdm(dataloaders['train'])\n",
        "# Finish your code here for 1.4. You may re-used most of your code for 1.1."
      ],
      "metadata": {
        "id": "RV8KX_vEQUnn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}