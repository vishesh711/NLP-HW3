{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vishesh711/NLP-HW3/blob/main/hw3_code_skeleton.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install the necessary libraries\n"
      ],
      "metadata": {
        "id": "57H-mVBq1d4v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCdFoYit1Iis"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "! pip install tqdm boto3 requests regex sentencepiece sacremoses\n",
        "! pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BERT Features\n",
        "\n",
        "In this part, you will use BERT features to classify DBPedia articles.\n",
        "The data is already pre-processed, and the data loader is implemented below."
      ],
      "metadata": {
        "id": "y2usTWPt2Tjs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Basics: dataset, data loaders, Classifier\n",
        "import collections\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import tqdm\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "\n",
        "SPLITS = ['train', 'dev', 'test']\n",
        "\n",
        "class DBPediaDataset(Dataset):\n",
        "  '''DBPedia dataset.\n",
        "    Args:\n",
        "      path[str]: path to the original data.\n",
        "  '''\n",
        "  def __init__(self, path):\n",
        "    with open(path) as fin:\n",
        "      self._data = [json.loads(l) for l in fin]\n",
        "    self._n_classes = len(set([datum['label'] for datum in self._data]))\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self._data[index]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self._data)\n",
        "\n",
        "  @property\n",
        "  def n_classes(self):\n",
        "    return self._n_classes\n",
        "\n",
        "  @staticmethod\n",
        "  def collate_fn(tokenizer, device, batch):\n",
        "    '''The collate function that compresses a training batch.\n",
        "      Args:\n",
        "        batch[list[dict[str, Any]]]: data in the batch.\n",
        "      Returns:\n",
        "        labels[torch.LongTensor]: the labels in the batch.\n",
        "        sentences[dict[str, torch.Tensor]]: sentences converted by tokenizers.\n",
        "    '''\n",
        "    labels = torch.tensor([datum['label'] for datum in batch]).long().to(device)\n",
        "    sentences = tokenizer(\n",
        "        [datum['sentence'] for datum in batch],\n",
        "        return_tensors='pt',  # pt = pytorch style tensor\n",
        "        padding=True)\n",
        "    for key in sentences:\n",
        "      sentences[key] = sentences[key].to(device)\n",
        "    return labels, sentences\n",
        "\n",
        "def construct_datasets(prefix, batch_size, tokenizer, device):\n",
        "  '''Constructs datasets and data loaders.\n",
        "    Args:\n",
        "      prefix[str]: prefix of the dataset (e.g., dbpedia_).\n",
        "      batch_size[int]: maximum number of examples in a batch.\n",
        "      tokenizer: model tokenizer that converts sentences to integer tensors.\n",
        "      device[torch.device]: the device (cpu/gpu) that the tensor should be on.\n",
        "    Returns:\n",
        "      datasets[dict[str, Dataset]]: a dict of constructed datasets.\n",
        "      dataloaders[dict[str, DataLoader]]: a dict of constructed data loaders.\n",
        "  '''\n",
        "  datasets = collections.defaultdict()\n",
        "  dataloaders = collections.defaultdict()\n",
        "  for split in SPLITS:\n",
        "    datasets[split] = DBPediaDataset(f'{prefix}{split}.json')\n",
        "    dataloaders[split] = DataLoader(\n",
        "        datasets[split],\n",
        "        batch_size=batch_size,\n",
        "        shuffle=(split == 'train'),\n",
        "        collate_fn=lambda x:DBPediaDataset.collate_fn(tokenizer, device, x))\n",
        "  return datasets, dataloaders"
      ],
      "metadata": {
        "id": "Sr_s2OH017B3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.1: [CODE] put your implementation of classifer here\n",
        "class Classifier(nn.Module):"
      ],
      "metadata": {
        "id": "x7zw6kKUE4FY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and Evaluation"
      ],
      "metadata": {
        "id": "isBytE8_OUSX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameters\n",
        "batch_size = 32\n",
        "classifier_hidden_size = 32\n",
        "# hyperparameters ends\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
        "bert_model = AutoModel.from_pretrained('bert-base-cased')\n",
        "if torch.cuda.is_available():  # use GPU if available\n",
        "  bert_model = bert_model.cuda()\n",
        "datasets, dataloaders = construct_datasets(\n",
        "    prefix='dbpedia_',\n",
        "    batch_size=batch_size,\n",
        "    tokenizer=tokenizer,\n",
        "    device=bert_model.device)\n",
        "\n",
        "classifier = Classifier(\n",
        "    bert_model.config.hidden_size,\n",
        "    classifier_hidden_size,\n",
        "    datasets['train'].n_classes).to(bert_model.device)\n",
        "optimizer = torch.optim.Adam(classifier.parameters(), lr=5e-4)\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "pbar = tqdm.tqdm(dataloaders['train'])\n",
        "for labels, sentences in pbar:\n",
        "  with torch.no_grad():\n",
        "    unpooled_features = bert_model(**sentences)['last_hidden_state'] # [B, L, D]\n",
        "  # 1.1: [CODE] train your classifier here\n",
        "\n",
        "  # 1.1: [CODE] ends here\n",
        "  # Note: you can re-use this code snippet for 1.2 as well"
      ],
      "metadata": {
        "id": "__xK_d_3pHuT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameters\n",
        "batch_size = 32\n",
        "classifier_hidden_size = 32\n",
        "# hyperparameters ends\n",
        "\n",
        "classifier = Classifier(\n",
        "    bert_model.config.hidden_size,\n",
        "    classifier_hidden_size,\n",
        "    datasets['train'].n_classes).to(bert_model.device)\n",
        "\n",
        "params = list()\n",
        "for name, param in bert_model.named_parameters():\n",
        "  if name.startswith... # 1.3: [CODE] this line is incomplete, you can finish this line by adding the last two layers' parameters to \"params\", or re-write your own code\n",
        "    params.append(param)\n",
        "optimizer = torch.optim.Adam(params + list(classifier.parameters()), lr=5e-4)\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "pbar = tqdm.tqdm(dataloaders['train'])\n",
        "# Finish your code here for 1.4. You may re-used most of your code for 1.1."
      ],
      "metadata": {
        "id": "RV8KX_vEQUnn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}